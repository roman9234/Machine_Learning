{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T17:12:34.711835Z",
     "start_time": "2025-04-10T17:12:34.708005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "import random\n",
    "import os\n",
    "# Убиарем предупреждение о симулинках\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ],
   "id": "129e5a325e181711",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T17:16:19.264105Z",
     "start_time": "2025-04-10T17:12:34.749130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Если возможно, пытаемся запустить на видеокарте\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Загрузка данных\n",
    "data_path = \"data/labeled.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "\n",
    "# Используем токенизатор Hugging Face\n",
    "# Токенизатор Hugging Face — это инструмент из библиотеки Transformers, который преобразует текст в числовой формат\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-base\")\n",
    "\n",
    "# Максимальная длина последовательности и размер батч\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# кастомный датасет\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        comment = row[\"comment\"]\n",
    "        # Строка преобразует метку label из столбца toxic (0 / 1) в тензор PyTorch с типом данных float32\n",
    "        # Это нужно чтобы метки можно было использовать в вычислениях модели\n",
    "        label = torch.tensor(row[\"toxic\"], dtype=torch.float32)\n",
    "\n",
    "        # Токенизация текста\n",
    "        encoding = self.tokenizer(\n",
    "            # текст токенизации\n",
    "            comment,\n",
    "            # Дополняем последовательности до максимальной длины\n",
    "            padding=\"max_length\",\n",
    "            # Обрезаем текст до максимальной длины\n",
    "            truncation=True,\n",
    "            # Максимальная длина текста\n",
    "            max_length=self.max_len,\n",
    "            # Возвращаем результат в виде PyTorch тензоров\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # извлекаем из результата токенизации два ключевых тензора:\n",
    "\n",
    "        # индексы токенов текста.\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        # маска внимания. указывает модели, какие токены учитывать\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            # Возвращаем текст в виде токенов\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": label,\n",
    "        }\n",
    "\n",
    "\n",
    "# Создаем датасет\n",
    "dataset = CustomDataSet(df, tokenizer, MAX_LEN)\n",
    "\n",
    "# Разделяем на обучающую и тестовую выборки 80 / 20 используя random_split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Используем даталоадеры\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# моедль GRU\n",
    "class LabModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LabModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embedded = self.embedding(input_ids)  # [batch_size, seq_len, embedding_dim]\n",
    "        _, hidden = self.gru(embedded)  # hidden: [1, batch_size, hidden_dim]\n",
    "        output = self.fc(hidden.squeeze(0))  # [batch_size, output_dim]\n",
    "        return self.sigmoid(output)\n",
    "\n",
    "\n",
    "# Параметры модели\n",
    "\n",
    "# Количество уникальных токенов в словаре\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "# Размер эмбеддингов (векторов слов)\n",
    "EMBEDDING_DIM = 128\n",
    "# Размер скрытого слоя GRU\n",
    "HIDDEN_DIM = 64\n",
    "# Размер выходного слоя (1 для бинарной классификации)\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = LabModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "\n",
    "# Оптимизатор отвечает за обновление параметров модели во время обучения.\n",
    "optimizer = torch.optim.Adam(\n",
    "    # список параметров модели\n",
    "    model.parameters(),\n",
    "    # скорость обучения learning rate\n",
    "    lr=3e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# обучение\n",
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "        # Прогоняем данные через модель\n",
    "        outputs = model(input_ids, attention_mask).squeeze(1)\n",
    "        # Вычисляем ошибку\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Вычисляем градиенты\n",
    "        loss.backward()\n",
    "        # Обновляем параметры модели\n",
    "        optimizer.step()\n",
    "\n",
    "        # Суммируем потери\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# валидация\n",
    "def _evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask).squeeze(1)  # Предсказанные вероятности\n",
    "            loss = criterion(outputs, labels)                     # Вычисляем ошибку\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Сохраняем истинные метки и предсказания\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(outputs.cpu().numpy())\n",
    "            # Классификация: вероятности > 0.5 -> класс 1\n",
    "            all_preds.extend((outputs > 0.5).cpu().numpy())\n",
    "\n",
    "    # Рассчитываем метрики\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(all_labels, all_probs)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "    return total_loss / len(dataloader), accuracy, f1, roc_auc, pr_auc\n",
    "\n",
    "\n",
    "# обучение\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_accuracy, val_f1, _, _ = _evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    # print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")"
   ],
   "id": "831439e03ba90418",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Accuracy: 0.6490\n",
      "F1: 0.0000\n",
      "ROC-AUC: 0.5776\n",
      "PR-AUC: 0.3979\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6373\n",
      "Accuracy: 0.6490\n",
      "F1: 0.0000\n",
      "ROC-AUC: 0.6187\n",
      "PR-AUC: 0.4600\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6314\n",
      "Accuracy: 0.7825\n",
      "F1: 0.6709\n",
      "ROC-AUC: 0.8236\n",
      "PR-AUC: 0.7350\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5775\n",
      "Accuracy: 0.8054\n",
      "F1: 0.6857\n",
      "ROC-AUC: 0.8624\n",
      "PR-AUC: 0.7930\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4189\n",
      "Accuracy: 0.8293\n",
      "F1: 0.7270\n",
      "ROC-AUC: 0.8820\n",
      "PR-AUC: 0.8204\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3294\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T17:24:02.611495Z",
     "start_time": "2025-04-10T17:24:02.603913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Проверка работы на случайных данных\n",
    "def predict_random_sample(model, dataset, amount = 5):\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(amount):\n",
    "        sample_idx = random.randint(0, len(dataset) - 1)\n",
    "        sample = dataset[sample_idx]\n",
    "\n",
    "        input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)\n",
    "        attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attention_mask).item()\n",
    "\n",
    "        prediction = \"Токсичный\" if output > 0.5 else \"Не токсичный\"\n",
    "        print()\n",
    "        print(f\"Настоящий тип: {'Токсичный' if df.iloc[sample_idx]['toxic'] == 1.0 else 'Не токсичный'} | Предсказание: {prediction}\")\n",
    "        print(f\"Комментарий: {df.iloc[sample_idx]['comment']}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "# Проверка работы на заданных вручную данных:\n",
    "def predict_manual_input(_model, _tokenizer, text, device):\n",
    "\n",
    "    # Ввод данных от пользователя\n",
    "    columns = [\"comment\",\"toxic\"]\n",
    "    data = [[text,0]]\n",
    "\n",
    "    # Создание DataFrame\n",
    "    _df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    sample_idx = 0\n",
    "\n",
    "    _dataset = CustomDataSet(_df, _tokenizer, MAX_LEN)\n",
    "    sample = _dataset[sample_idx]\n",
    "\n",
    "    input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)\n",
    "    attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = _model(input_ids, attention_mask).item()\n",
    "\n",
    "    prediction = \"Токсичный\" if output > 0.5 else \"Не токсичный\"\n",
    "    print()\n",
    "    print(f\"Предсказание: {prediction}\")\n",
    "    print(f\"Комментарий: {_df.iloc[sample_idx]['comment']}\")\n",
    "\n",
    "def predict_programm(_model, _tokenizer, device):\n",
    "    while True:\n",
    "        q = str(input())\n",
    "        if q in [\"q\", \"quit\", \"\"]:\n",
    "            break\n",
    "        else:\n",
    "            predict_manual_input(_model, _tokenizer, q, device)"
   ],
   "id": "9d6e91b522fd6417",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T17:33:41.149660Z",
     "start_time": "2025-04-10T17:33:41.131660Z"
    }
   },
   "cell_type": "code",
   "source": "predict_random_sample(model, dataset, 2)",
   "id": "7ea54e75a88c9ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Настоящий тип: Не токсичный | Предсказание: Не токсичный\n",
      "Комментарий: Говорят в администрации президента вызвали учителя из Китая....\n",
      "\n",
      "\n",
      "\n",
      "Настоящий тип: Токсичный | Предсказание: Токсичный\n",
      "Комментарий: Так высрал же тебе на лицо, а ты утерся и отреагировал, ну че ты как маленький.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T17:33:47.883779Z",
     "start_time": "2025-04-10T17:33:45.176398Z"
    }
   },
   "cell_type": "code",
   "source": "predict_programm(model, tokenizer, device)",
   "id": "ca575de598e3701",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Предсказание: Не токсичный\n",
      "Комментарий: Автор всё правильно сказал, я с ним согласен\n"
     ]
    }
   ],
   "execution_count": 61
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
