{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-13T20:25:44.163624Z",
     "start_time": "2025-03-13T20:25:42.000612Z"
    }
   },
   "source": [
    "!python.exe -m pip install --upgrade pip\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import sent_tokenize\n",
    "import os\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (25.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\roman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\roman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\roman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T20:29:53.986947Z",
     "start_time": "2025-03-13T20:28:10.319594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Функция для очистки текста: удаление цифр, римских цифр, стоп-слов и лемматизация.\n",
    "    \"\"\"\n",
    "    # Инициализация инструментов\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))  # Стоп-слова на английском\n",
    "\n",
    "    # Удаляем римские цифры\n",
    "    text = re.sub(r'\\b[IVXLCDM]+\\b', '', text, flags=re.IGNORECASE)\n",
    "    # Удаляем обычные цифры\n",
    "    text = re.sub(r'd+', '', text)\n",
    "    # Удаляем _\n",
    "    text = re.sub('_', '', text)\n",
    "\n",
    "    # Токенизация текста\n",
    "    tokens = word_tokenize(text)\n",
    "    # Удаляем стоп-слова и проводим лемматизацию (приведение к базовой форме слова)\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(token.lower())  # Лемматизация и приведение к нижнему регистру\n",
    "        for token in tokens\n",
    "        if token.isalpha() and token.lower() not in stop_words  # Убираем неалфавитные символы и стоп-слова\n",
    "    ]\n",
    "\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "\n",
    "def process_files_for_tfidf_and_word2vec(directory_path):\n",
    "    \"\"\"\n",
    "    Функция читает все .txt файлы из указанной директории,\n",
    "    выполняет очистку текста, фильтрует абзацы по длине,\n",
    "    применяет к тексту функцию clean_text\n",
    "    \"\"\"\n",
    "    list_of_paragraphs = []  # Для TF-IDF\n",
    "    list_of_sentences = []   # Для Word2Vec\n",
    "\n",
    "    # Обходим все файлы в директории\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Обрабатываем только файлы с расширением .txt\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            # Открываем файл\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "\n",
    "                # Разделяем текст на абзацы по переносам строки (\\n)\n",
    "                paragraphs = text.split('\\n\\n')\n",
    "                paragraphs = [p.strip() for p in paragraphs if p.strip()]  # Убираем пустые строки\n",
    "\n",
    "                # Очищаем каждый абзац\n",
    "                cleaned_paragraphs = []\n",
    "                for paragraph in paragraphs:\n",
    "                    cleaned_paragraph = clean_text(paragraph)  # Очистка текста\n",
    "                    if len(cleaned_paragraph) >= 100:  # Фильтруем абзацы меньше 100 символов\n",
    "                        cleaned_paragraphs.append(cleaned_paragraph)\n",
    "\n",
    "                list_of_paragraphs.extend(cleaned_paragraphs)\n",
    "\n",
    "                # Разделяем текст на предложения для Word2Vec\n",
    "                sentences = sent_tokenize(text)  # Используем NLTK для разделения на предложения\n",
    "                for sentence in sentences:\n",
    "                    cleaned_sentence = clean_text(sentence)  # Очистка текста\n",
    "                    tokenized_sentence = word_tokenize(cleaned_sentence)  # Токенизация предложения\n",
    "                    if tokenized_sentence:  # Проверяем, что предложение не пустое после очистки\n",
    "                        list_of_sentences.append(tokenized_sentence)\n",
    "\n",
    "    return list_of_paragraphs, list_of_sentences\n",
    "\n",
    "\n",
    "# Пример использования:\n",
    "directory_path = \"texts/\"  # Укажите путь к директории с .txt файлами\n",
    "\n",
    "# Получаем данные для TF-IDF и Word2Vec\n",
    "paragraphs_for_tfidf, sentences_for_word2vec = process_files_for_tfidf_and_word2vec(directory_path)\n",
    "\n",
    "# Выводим результаты\n",
    "print(\"Количество абзацев для TF-IDF:\", len(paragraphs_for_tfidf))\n",
    "print(\"Пример абзаца:\", paragraphs_for_tfidf[0])\n",
    "print(\"\\nКоличество предложений для Word2Vec:\", len(sentences_for_word2vec))\n",
    "print(\"Пример токенизированного предложения:\", sentences_for_word2vec[0])\n",
    "\n",
    "# Если нужно обучить TF-IDF или Word2Vec:\n",
    "# Пример создания TF-IDF модели:\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(paragraphs_for_tfidf)\n",
    "print(\"\\nTF-IDF матрица создана. Размер:\", tfidf_matrix.shape)\n",
    "\n",
    "# Пример обучения Word2Vec модели:\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=sentences_for_word2vec, vector_size=100, window=5, min_count=1, workers=4)\n",
    "print(\"\\nWord2Vec модель обучена.\")"
   ],
   "id": "fc492dbbf1534f36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество абзацев для TF-IDF: 21906\n",
      "Пример абзаца: sai name though common name efinition corresponing name iffers thus real man figure picture lay claim name yet equivocally name though common name efinition corresponing name iffers shoul one efine sense animal efinition one case appropriate case\n",
      "\n",
      "Количество предложений для Word2Vec: 127111\n",
      "Пример токенизированного предложения: ['sai', 'name', 'though', 'common', 'name', 'efinition', 'corresponing', 'name', 'iffers']\n",
      "\n",
      "TF-IDF матрица создана. Размер: (21906, 37105)\n",
      "\n",
      "Word2Vec модель обучена.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T20:37:50.726499Z",
     "start_time": "2025-03-13T20:37:50.713855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_similar(_similar_words, _max = 3, none_words = []):\n",
    "    # это список кортежей, где каждый кортеж содержит слово и его косинусное сходство\n",
    "    i = 0\n",
    "    for _word, _similarity in _similar_words:\n",
    "        if _word not in none_words:\n",
    "            i+=1\n",
    "            print(f\"{_word}: {_similarity:.4f}\")\n",
    "            if i >= _max:\n",
    "                break\n",
    "\n",
    "\n",
    "def get_result_vector(model, add_words=[], substract_words=[]):\n",
    "    result_vector = np.zeros(model.vector_size)\n",
    "    for word in add_words:\n",
    "        if word in model.wv:\n",
    "            result_vector += model.wv[word]\n",
    "    for word in substract_words:\n",
    "        if word in model.wv:\n",
    "            result_vector -= model.wv[word]\n",
    "\n",
    "    return result_vector\n",
    "\n",
    "def get_similar_words(model, add_words=[], substract_words=[]):\n",
    "    if len(substract_words) > 0:\n",
    "        print(\" + \".join(add_words) + \" - \" + \" - \".join(substract_words))\n",
    "    else:\n",
    "        print(\" + \".join(add_words))\n",
    "    _result_vector = get_result_vector(model, add_words, substract_words)\n",
    "    _similar_words = word2vec_model.wv.similar_by_vector(_result_vector)\n",
    "    print_similar(_similar_words, none_words=add_words+substract_words)\n",
    "    print()\n",
    "\n",
    "get_similar_words(word2vec_model, [\"order\", \"people\"], [\"chaos\"])\n",
    "get_similar_words(word2vec_model, [\"rage\", \"anger\", \"violence\"], [\"good\"])\n",
    "get_similar_words(word2vec_model, [\"king\", \"woman\"], [\"man\"])\n",
    "get_similar_words(word2vec_model, [\"defeat\", \"war\"])"
   ],
   "id": "688ceba045c8b441",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order + people - chaos\n",
      "nation: 0.6912\n",
      "men: 0.6299\n",
      "foreigner: 0.6293\n",
      "\n",
      "rage + anger + violence - good\n",
      "expose: 0.8646\n",
      "terror: 0.8556\n",
      "injury: 0.8552\n",
      "\n",
      "king + woman - man\n",
      "queen: 0.7404\n",
      "henry: 0.7100\n",
      "pope: 0.6984\n",
      "\n",
      "defeat + war\n",
      "conquest: 0.7875\n",
      "peace: 0.7868\n",
      "invasion: 0.7557\n",
      "\n"
     ]
    }
   ],
   "execution_count": 53
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
