{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1 лабораторная работа - \"word2vec и TF-IDF\". БВТ2202 Лесовой Роман Сергеевич",
   "id": "5552f37a6f25b71d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-14T14:39:41.910460Z",
     "start_time": "2025-03-14T14:39:23.246435Z"
    }
   },
   "source": [
    "!python.exe -m pip install --upgrade pip\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import sent_tokenize\n",
    "import os\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (25.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\roman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\roman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\roman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:48:58.859455Z",
     "start_time": "2025-03-14T14:47:08.047392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Функция для очистки текста\n",
    "    \"\"\"\n",
    "    # Инициализация инструментов\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))  # Стоп-слова на английском\n",
    "\n",
    "    # Удаляем римские цифры\n",
    "    text = re.sub(r'\\b[IVXLCDM]+\\b', '', text, flags=re.IGNORECASE)\n",
    "    # Удаляем обычные цифры\n",
    "    text = re.sub(r'd+', '', text)\n",
    "    # Удаляем _\n",
    "    text = re.sub('_', '', text)\n",
    "\n",
    "    # Токенизация текста\n",
    "    tokens = word_tokenize(text)\n",
    "    # Удаляем стоп-слова и проводим лемматизацию (приведение к базовой форме слова)\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(token.lower())  # Лемматизация и приведение к нижнему регистру\n",
    "        for token in tokens\n",
    "        if token.isalpha() and token.lower() not in stop_words  # Убираем неалфавитные символы и стоп-слова\n",
    "    ]\n",
    "\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "\n",
    "def process_files(directory_path):\n",
    "    \"\"\"\n",
    "    Функция читает все .txt файлы из указанной директории,\n",
    "    выполняет очистку текста, фильтрует абзацы по длине,\n",
    "    применяет к тексту функцию clean_text\n",
    "    \"\"\"\n",
    "    # Для TF-IDF\n",
    "    list_of_paragraphs = []\n",
    "    # Для Word2Vec\n",
    "    list_of_sentences = []\n",
    "\n",
    "    # Обходим все файлы в директории\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "\n",
    "                # Разделяем на абзацы\n",
    "                paragraphs = text.split('\\n\\n')\n",
    "                paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "\n",
    "                # Очищаем каждый абзац\n",
    "                cleaned_paragraphs = []\n",
    "                for paragraph in paragraphs:\n",
    "                    cleaned_paragraph = clean_text(paragraph)  # Очистка текста\n",
    "                    # Не берём слишком короткие абзацы\n",
    "                    if len(cleaned_paragraph) >= 100:\n",
    "                        cleaned_paragraphs.append(cleaned_paragraph)\n",
    "\n",
    "                list_of_paragraphs.extend(cleaned_paragraphs)\n",
    "\n",
    "                # Разделяем текст на предложения для Word2Vec\n",
    "                # Используем NLTK для разделения на предложения\n",
    "                sentences = sent_tokenize(text)\n",
    "                for sentence in sentences:\n",
    "                    cleaned_sentence = clean_text(sentence)  # Очистка текста\n",
    "                    tokenized_sentence = word_tokenize(cleaned_sentence)  # Токенизация предложения\n",
    "                    if tokenized_sentence:  # Проверяем, что предложение не пустое после очистки\n",
    "                        list_of_sentences.append(tokenized_sentence)\n",
    "\n",
    "    return list_of_paragraphs, list_of_sentences\n",
    "\n",
    "\n",
    "\n",
    "# Путь к текстам\n",
    "directory_path = \"texts/\"\n",
    "\n",
    "# Данные для TF-IDF и Word2Vec\n",
    "paragraphs_for_tfidf, sentences_for_word2vec = process_files(directory_path)\n",
    "\n",
    "# Вывод полученных данных\n",
    "print(\"Количество абзацев TF-IDF:\", len(paragraphs_for_tfidf))\n",
    "print(\"Пример абзаца:\", paragraphs_for_tfidf[0])\n",
    "print(\"\\nКоличество предложений для Word2Vec:\", len(sentences_for_word2vec))\n",
    "print(\"Пример предложения:\", sentences_for_word2vec[0])\n",
    "\n",
    "# создание TF-IDF модели\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(paragraphs_for_tfidf)\n",
    "print(\"\\nTF-IDF матрица создана. Размер матрицы:\", tfidf_matrix.shape)\n",
    "\n",
    "# обучение Word2Vec модели\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=sentences_for_word2vec, vector_size=100, window=5, min_count=1, workers=4)\n",
    "print(\"\\nWord2Vec модель обучена.\")"
   ],
   "id": "fc492dbbf1534f36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество абзацев TF-IDF: 21906\n",
      "Пример абзаца: sai name though common name efinition corresponing name iffers thus real man figure picture lay claim name yet equivocally name though common name efinition corresponing name iffers shoul one efine sense animal efinition one case appropriate case\n",
      "\n",
      "Количество предложений для Word2Vec: 127111\n",
      "Пример предложения: ['sai', 'name', 'though', 'common', 'name', 'efinition', 'corresponing', 'name', 'iffers']\n",
      "\n",
      "TF-IDF матрица создана. Размер матрицы: (21906, 37105)\n",
      "\n",
      "Word2Vec модель обучена.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T15:02:46.254081Z",
     "start_time": "2025-03-14T15:02:46.207317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# первые 10 слов и их TF-IDF значения для первого абзаца:\n",
    "\n",
    "# Получаем список всех слов (фичей)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "# Берем первый абзац из TF-IDF матрицы\n",
    "first_paragraph_tfidf = tfidf_matrix[0]\n",
    "# Преобразуем в массив\n",
    "first_paragraph_tfidf_dense = first_paragraph_tfidf.toarray()[0]\n",
    "\n",
    "# Сопоставляем слова с их TF-IDF значениями\n",
    "word_tfidf_pairs = [(feature_names[i], first_paragraph_tfidf_dense[i]) for i in range(len(feature_names)) if first_paragraph_tfidf_dense[i] > 0]\n",
    "sorted_word_tfidf_pairs = sorted(word_tfidf_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# топ 10 слов с наибольшими значениями TF-IDF\n",
    "print(\"\\nТоп-10 слов с наибольшими TF-IDF значениями в первом абзаце:\")\n",
    "for word, score in sorted_word_tfidf_pairs[:10]:\n",
    "    print(f\"{word}: {score:.4f}\")"
   ],
   "id": "28ebb23842f96cf2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Топ-10 слов с наибольшими TF-IDF значениями в первом абзаце:\n",
      "name: 0.6218\n",
      "efinition: 0.4115\n",
      "iffers: 0.3055\n",
      "corresponing: 0.2831\n",
      "equivocally: 0.2220\n",
      "common: 0.1609\n",
      "case: 0.1502\n",
      "appropriate: 0.1463\n",
      "picture: 0.1389\n",
      "efine: 0.1345\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T15:09:29.431368Z",
     "start_time": "2025-03-14T15:09:29.418027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_similar(_similar_words, _max = 3, none_words = []):\n",
    "    # это список кортежей, где каждый кортеж содержит слово и его косинусное сходство\n",
    "    i = 0\n",
    "    for _word, _similarity in _similar_words:\n",
    "        if _word not in none_words:\n",
    "            i+=1\n",
    "            print(f\"{_word}: {_similarity:.4f}\")\n",
    "            if i >= _max:\n",
    "                break\n",
    "\n",
    "\n",
    "def get_result_vector(model, add_words=[], substract_words=[]):\n",
    "    result_vector = np.zeros(model.vector_size)\n",
    "    for word in add_words:\n",
    "        if word in model.wv:\n",
    "            result_vector += model.wv[word]\n",
    "    for word in substract_words:\n",
    "        if word in model.wv:\n",
    "            result_vector -= model.wv[word]\n",
    "\n",
    "    return result_vector\n",
    "\n",
    "def get_similar_words(model, add_words=[], substract_words=[]):\n",
    "    if len(substract_words) > 0:\n",
    "        print(\" + \".join(add_words) + \" - \" + \" - \".join(substract_words))\n",
    "    else:\n",
    "        print(\" + \".join(add_words))\n",
    "    _result_vector = get_result_vector(model, add_words, substract_words)\n",
    "    _similar_words = word2vec_model.wv.similar_by_vector(_result_vector)\n",
    "    print_similar(_similar_words, none_words=add_words+substract_words)\n",
    "    print()\n",
    "\n",
    "get_similar_words(word2vec_model, [\"order\", \"people\"], [\"chaos\"])\n",
    "get_similar_words(word2vec_model, [\"rage\", \"anger\", \"violence\"], [\"good\"])\n",
    "get_similar_words(word2vec_model, [\"king\", \"woman\"], [\"man\"])\n",
    "get_similar_words(word2vec_model, [\"defeat\", \"war\"])"
   ],
   "id": "688ceba045c8b441",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order + people - chaos\n",
      "nation: 0.7072\n",
      "men: 0.6405\n",
      "mankin: 0.6371\n",
      "\n",
      "rage + anger + violence - good\n",
      "expose: 0.8865\n",
      "terror: 0.8671\n",
      "violent: 0.8605\n",
      "\n",
      "king + woman - man\n",
      "queen: 0.7351\n",
      "henry: 0.7266\n",
      "pope: 0.6806\n",
      "\n",
      "defeat + war\n",
      "peace: 0.8152\n",
      "conquest: 0.8054\n",
      "wage: 0.7647\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
