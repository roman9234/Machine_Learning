{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Предобработка текста\n",
    "\n",
    "Тексты нужно подготовить перед использованием:\n",
    "\n",
    "Привести к нижнему регистру.\n",
    "\n",
    "Удалить пунктуацию и стоп-слова.\n",
    "\n",
    "Токенизировать текст (разбить на слова)."
   ],
   "id": "35c3b1022ddb3156"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "777366b86e58b2be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T19:56:49.816438Z",
     "start_time": "2025-03-13T19:56:45.945794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install gensim scikit-learn nltk pandas\n",
    "!python.exe -m pip install --upgrade pip"
   ],
   "id": "4985ad46c23fcb3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: wrapt in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: colorama in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: pip in c:\\programming_hub\\machine_learning\\venv\\lib\\site-packages (25.0.1)\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T19:56:49.828406Z",
     "start_time": "2025-03-13T19:56:49.822449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "id": "a5228785e7f414d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\roman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\roman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\roman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T19:56:51.390197Z",
     "start_time": "2025-03-13T19:56:49.840701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def df_fix(_df, _column_name):\n",
    "    # Удаление строк с пропущенными значениями\n",
    "    _df[_column_name] = _df[_column_name].str.lower().str.strip()\n",
    "    # Нижний регистр, удаление лишних пробелов\n",
    "    _df[_column_name] = _df[_column_name].str.replace('\\n', '', regex=False)\n",
    "    return _df\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(text)\n",
    "    # Удаление пунктуации и стоп-слов\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    # Лемматизация\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Загрузка из CSV\n",
    "df = pd.read_csv('commonlit_texts.csv')\n",
    "\n",
    "columns_to_keep = ['title', 'description', 'is_prose', 'genre', 'intro']\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "df = df.dropna()\n",
    "df_fix(df, 'title')\n",
    "df_fix(df, 'description')\n",
    "df_fix(df, 'genre')\n",
    "df_fix(df, 'intro')\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df.sample(n=1)[\"intro\"])\n",
    "\n",
    "df['processed_intro'] = df['intro'].apply(preprocess_text)"
   ],
   "id": "60f63033992c1129",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696    when franklin delano roosevelt was elected president of the united states on march 4, 1933, the country was in the grip of the great depression. at his inauguration, roosevelt delivered the following famous speech, in which he addresses the growing fear that plagued a nation in crisis.as you read, identify roosevelt's purpose in the speech and take notes on the rhetorical techniques he uses to make his points.\n",
      "Name: intro, dtype: object\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Построение TF-IDF",
   "id": "27a50b738050819d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T19:56:51.487607Z",
     "start_time": "2025-03-13T19:56:51.411273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Построение TF-IDF\n",
    "# Создание TF-IDF векторизатора\n",
    "# Ограничим до 5000 слов для простоты\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df['processed_intro'])\n",
    "\n",
    "# Посмотрим на матрицу TF-IDF\n",
    "# (количество документов, количество уникальных слов)\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "# Получение словаря TF-IDF\n",
    "# print(vectorizer.get_feature_names_out())\n",
    "# print(tfidf_matrix.toarray())"
   ],
   "id": "b3649953c2ab7d19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 5000)\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " Построение Word2Vec",
   "id": "ed96ee7564fa9ae3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T19:56:51.876101Z",
     "start_time": "2025-03-13T19:56:51.502635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Токенизация текста для Word2Vec\n",
    "tokenized_intros = df['processed_intro'].apply(lambda x: x.split())\n",
    "\n",
    "# Создание модели Word2Vec\n",
    "w2v_model = Word2Vec(sentences=tokenized_intros, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Получение вектора для слова\n",
    "word_vector = w2v_model.wv['example']  # Вектор для слова 'example'\n",
    "print(word_vector)"
   ],
   "id": "ceff02d59821054f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12865202  0.16068581 -0.00903208  0.01996073 -0.00204957 -0.35496235\n",
      "  0.13322583  0.5026308  -0.13195437 -0.11418425 -0.09907986 -0.34955359\n",
      " -0.04566629  0.06991561  0.0721117  -0.19215833  0.09035921 -0.21170096\n",
      "  0.01925341 -0.37455475  0.15248346  0.03460553  0.23816903 -0.09778078\n",
      " -0.0336967   0.03645394 -0.12793027 -0.01531199 -0.22777236  0.03337726\n",
      "  0.2744209  -0.04598208  0.06480784 -0.15564622 -0.0332095   0.20020145\n",
      "  0.01436602 -0.09464411 -0.12269801 -0.28855905  0.0778457  -0.16574652\n",
      " -0.12166671  0.0539104   0.16844968 -0.09749313 -0.17437445 -0.10189272\n",
      "  0.06633928  0.18270305  0.0743015  -0.19529557 -0.02657109 -0.04388666\n",
      " -0.15187907  0.07686915  0.13252892 -0.02945541 -0.17664932  0.08361544\n",
      " -0.04563026 -0.01457122  0.08071989  0.05775671 -0.26300716  0.18690477\n",
      "  0.0389752   0.17435873 -0.29269823  0.28706557 -0.11392553  0.17251365\n",
      "  0.2680112  -0.06579765  0.21378314  0.04702346  0.12212237 -0.01939645\n",
      " -0.15129063  0.03197882 -0.2222559  -0.1613214  -0.084991    0.2922423\n",
      "  0.03372725  0.0159714   0.00764561  0.20691179  0.20979726  0.02536434\n",
      "  0.19172594  0.15903808  0.04809487 -0.00814053  0.3894826   0.21177287\n",
      "  0.16129994 -0.18505561  0.01738842  0.0531794 ]\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Комбинирование TF-IDF и Word2Vec",
   "id": "8da9825c65b93d42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T19:56:58.423035Z",
     "start_time": "2025-03-13T19:56:51.882111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# После того как у нас есть TF-IDF и Word2Vec, мы можем объединить их. Например:\n",
    "\n",
    "# Для каждого документа можно вычислить взвешенную сумму Word2Vec-векторов слов, где веса — это значения TF-IDF.\n",
    "\n",
    "def document_vector(doc, model, tfidf_vectorizer):\n",
    "    # Получаем веса TF-IDF для всех слов документа\n",
    "    tfidf_weights = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.transform([' '.join(doc)]).toarray()[0]))\n",
    "\n",
    "    # Считаем вектор документа\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    for word in doc:\n",
    "        if word in model.wv and word in tfidf_weights:\n",
    "            vector += model.wv[word] * tfidf_weights[word]\n",
    "    return vector\n",
    "\n",
    "df['document_vector'] = df['processed_intro'].apply(lambda x: document_vector(x.split(), w2v_model, vectorizer))"
   ],
   "id": "cb324c2a8f402a89",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Операции со словами",
   "id": "af7ae55084e3521f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T19:56:58.444845Z",
     "start_time": "2025-03-13T19:56:58.436118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_similar(_similar_words):\n",
    "    # это список кортежей, где каждый кортеж содержит слово и его косинусное сходство\n",
    "    for _word, _similarity in _similar_words:\n",
    "        print(f\"{_word}: {_similarity:.4f}\")\n",
    "\n",
    "def get_result_vector(model, add_words=[], substract_words=[]):\n",
    "    result_vector = np.zeros(model.vector_size)\n",
    "    for word in add_words:\n",
    "        if word in model.wv:\n",
    "            result_vector += model.wv[word]\n",
    "    for word in substract_words:\n",
    "        if word in model.wv:\n",
    "            result_vector -= model.wv[word]\n",
    "\n",
    "    return result_vector\n",
    "\n",
    "\n",
    "# Пример: король - мужчина + женщина\n",
    "# result_vector = get_result_vector(w2v_model, [\"king\", \"woman\"], [\"man\"])\n",
    "# similar_words = w2v_model.wv.similar_by_vector(result_vector)\n",
    "# print_similar(similar_words)\n",
    "\n",
    "result_vector = get_result_vector(w2v_model, [\"family\", \"male\", \"old\"], [])\n",
    "similar_words = w2v_model.wv.similar_by_vector(result_vector)\n",
    "print_similar(similar_words)"
   ],
   "id": "2f0b2f18a17c053b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family: 0.9999\n",
      "student: 0.9998\n",
      "made: 0.9998\n",
      "called: 0.9998\n",
      "day: 0.9998\n",
      "play: 0.9998\n",
      "group: 0.9998\n",
      "human: 0.9998\n",
      "new: 0.9998\n",
      "school: 0.9998\n"
     ]
    }
   ],
   "execution_count": 86
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
