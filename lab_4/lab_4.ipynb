{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-07T12:52:15.706153Z",
     "start_time": "2025-05-07T12:47:48.450429Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "\n",
    "# Если возможно, пытаемся запустить на видеокарте\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Загрузка данных\n",
    "data_path = \"data/labeled.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Используем токенизатор Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-base\")\n",
    "\n",
    "# Максимальная длина последовательности и размер батч\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Кастомный датасет\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        comment = row[\"comment\"]\n",
    "        label = torch.tensor(row[\"toxic\"], dtype=torch.long)  # Для BERT метки должны быть типа long\n",
    "\n",
    "        # Токенизация текста\n",
    "        encoding = self.tokenizer(\n",
    "            comment,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": label,\n",
    "        }\n",
    "\n",
    "new_size = 0.2\n",
    "reduced_df = df.sample(frac=new_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Создаем датасет\n",
    "dataset = CustomDataSet(reduced_df, tokenizer, MAX_LEN)\n",
    "\n",
    "# Разделяем на обучающую и тестовую выборки 80 / 20 используя random_split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Используем DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Используем предобученную модель BERT для классификации (1 класс — токсичность)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"ai-forever/ruBert-base\", num_labels=2  # Для бинарной классификации (0 или 1)\n",
    ").to(device)\n",
    "\n",
    "# Оптимизатор и функция потерь\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Функция для обучения модели\n",
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Общее количество батчей в даталоадере\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Прямой проход через модель\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss  # Loss возвращается автоматически в BertForSequenceClassification\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Обратное распространение ошибки\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Выводим количество оставшихся батчей\n",
    "        remaining_batches = total_batches - (batch_idx + 1)\n",
    "        print(f\"Batch {batch_idx + 1}/{total_batches} completed. Remaining batches: {remaining_batches}\")\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# Основной цикл обучения и тестирования\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"started training\")\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion)\n",
    "    print(\"initicated train_model()\")\n",
    "    # val_accuracy = evaluate_model(model, test_loader)\n",
    "    print(\"initicated evaluate_model()\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    # print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started training\n",
      "Batch 1/36 completed. Remaining batches: 35\n",
      "Batch 2/36 completed. Remaining batches: 34\n",
      "Batch 3/36 completed. Remaining batches: 33\n",
      "Batch 4/36 completed. Remaining batches: 32\n",
      "Batch 5/36 completed. Remaining batches: 31\n",
      "Batch 6/36 completed. Remaining batches: 30\n",
      "Batch 7/36 completed. Remaining batches: 29\n",
      "Batch 8/36 completed. Remaining batches: 28\n",
      "Batch 9/36 completed. Remaining batches: 27\n",
      "Batch 10/36 completed. Remaining batches: 26\n",
      "Batch 11/36 completed. Remaining batches: 25\n",
      "Batch 12/36 completed. Remaining batches: 24\n",
      "Batch 13/36 completed. Remaining batches: 23\n",
      "Batch 14/36 completed. Remaining batches: 22\n",
      "Batch 15/36 completed. Remaining batches: 21\n",
      "Batch 16/36 completed. Remaining batches: 20\n",
      "Batch 17/36 completed. Remaining batches: 19\n",
      "Batch 18/36 completed. Remaining batches: 18\n",
      "Batch 19/36 completed. Remaining batches: 17\n",
      "Batch 20/36 completed. Remaining batches: 16\n",
      "Batch 21/36 completed. Remaining batches: 15\n",
      "Batch 22/36 completed. Remaining batches: 14\n",
      "Batch 23/36 completed. Remaining batches: 13\n",
      "Batch 24/36 completed. Remaining batches: 12\n",
      "Batch 25/36 completed. Remaining batches: 11\n",
      "Batch 26/36 completed. Remaining batches: 10\n",
      "Batch 27/36 completed. Remaining batches: 9\n",
      "Batch 28/36 completed. Remaining batches: 8\n",
      "Batch 29/36 completed. Remaining batches: 7\n",
      "Batch 30/36 completed. Remaining batches: 6\n",
      "Batch 31/36 completed. Remaining batches: 5\n",
      "Batch 32/36 completed. Remaining batches: 4\n",
      "Batch 33/36 completed. Remaining batches: 3\n",
      "Batch 34/36 completed. Remaining batches: 2\n",
      "Batch 35/36 completed. Remaining batches: 1\n",
      "Batch 36/36 completed. Remaining batches: 0\n",
      "initicated train_model()\n",
      "initicated evaluate_model()\n",
      "Epoch 1/1\n",
      "Train Loss: 0.4313\n",
      "Validation Accuracy: 0.9239\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T16:33:35.157428Z",
     "start_time": "2025-05-07T16:33:18.338233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "import torch\n",
    "\n",
    "# Функция для тестирования модели с дополнительными метриками\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Получаем вероятности классов с помощью softmax\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "            probabilities.extend(probs[:, 1])  # Вероятности для положительного класса (1)\n",
    "\n",
    "            # Прогнозы — это индексы максимального значения логитов\n",
    "            predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Метрика Accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Метрика F1\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "\n",
    "    # Метрика ROC-AUC (для бинарной классификации)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(true_labels, probabilities)\n",
    "    except ValueError:\n",
    "        roc_auc = None  # Если только один класс в данных, ROC-AUC нельзя вычислить\n",
    "\n",
    "    # Метрика PR-AUC (Precision-Recall AUC)\n",
    "    precision, recall, _ = precision_recall_curve(true_labels, probabilities)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    # Возвращаем все метрики\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"pr_auc\": pr_auc\n",
    "    }\n",
    "metrics = evaluate_model(model, test_loader)"
   ],
   "id": "23d7b12e4aa612aa",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T16:33:36.955702Z",
     "start_time": "2025-05-07T16:33:36.951702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RNN_metrics = {\n",
    "    'accuracy': 0.8293,\n",
    "    'f1': 0.7270,\n",
    "    'roc_auc': 0.8820,\n",
    "    'pr_auc': 0.8204}\n",
    "\n",
    "for x in metrics.keys():\n",
    "    print(f\"{x}(BERT): {metrics[x]:.3f}. RNN: {RNN_metrics[x]:.3f}. Разница  = {(metrics[x]/RNN_metrics[x]-1)*100:.2f}%\")"
   ],
   "id": "e4641dcfe85b6ff0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(BERT): 0.924. RNN: 0.829. Разница  = 11.40%\n",
      "f1(BERT): 0.875. RNN: 0.727. Разница  = 20.36%\n",
      "roc_auc(BERT): 0.975. RNN: 0.882. Разница  = 10.58%\n",
      "pr_auc(BERT): 0.942. RNN: 0.820. Разница  = 14.81%\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:00:09.566289Z",
     "start_time": "2025-05-07T13:00:09.558992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Проверка работы на случайных данных\n",
    "def predict_random_sample(model, dataset, amount=5):\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(amount):\n",
    "        sample_idx = random.randint(0, len(dataset) - 1)\n",
    "        sample = dataset[sample_idx]\n",
    "\n",
    "        input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)\n",
    "        attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Получаем logits из модели\n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = output.logits  # логиты модели (размерность: [1, 2])\n",
    "\n",
    "            # Преобразуем логиты в вероятности\n",
    "            probabilities = torch.softmax(logits, dim=-1)  # размерность: [1, 2]\n",
    "            probability_toxic = probabilities[0][1].item()  # вероятность для класса \"Токсичный\"\n",
    "\n",
    "        prediction = \"Токсичный\" if probability_toxic > 0.5 else \"Не токсичный\"\n",
    "        print()\n",
    "        print(f\"Настоящий тип: {'Токсичный' if df.iloc[sample_idx]['toxic'] == 1.0 else 'Не токсичный'} | Предсказание: {prediction}\")\n",
    "        print(f\"Комментарий: {df.iloc[sample_idx]['comment']}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Проверка работы на заданных вручную данных:\n",
    "def predict_manual_input(_model, _tokenizer, text, device):\n",
    "\n",
    "    # Ввод данных от пользователя\n",
    "    columns = [\"comment\", \"toxic\"]\n",
    "    data = [[text, 0]]\n",
    "\n",
    "    # Создание DataFrame\n",
    "    _df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    sample_idx = 0\n",
    "\n",
    "    _dataset = CustomDataSet(_df, _tokenizer, MAX_LEN)\n",
    "    sample = _dataset[sample_idx]\n",
    "\n",
    "    input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)\n",
    "    attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Получаем logits из SequenceClassifierOutput\n",
    "        output = _model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = output.logits  # логиты модели\n",
    "        probabilities = torch.softmax(logits, dim=-1)  # размерность: [1, 2]\n",
    "        probability_toxic = probabilities[0][1].item()  # вероятность для класса \"Токсичный\"\n",
    "\n",
    "    prediction = \"Токсичный\" if probability_toxic > 0.5 else \"Не токсичный\"\n",
    "    print()\n",
    "    print(f\"Предсказание: {prediction}\")\n",
    "    print(f\"Комментарий: {_df.iloc[sample_idx]['comment']}\")\n",
    "\n",
    "\n",
    "def predict_programm(_model, _tokenizer, device):\n",
    "    while True:\n",
    "        q = str(input())\n",
    "        if q in [\"q\", \"quit\", \"\"]:\n",
    "            break\n",
    "        else:\n",
    "            predict_manual_input(_model, _tokenizer, q, device)"
   ],
   "id": "cf3a89f24317111",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:01:06.896019Z",
     "start_time": "2025-05-07T13:01:06.754937Z"
    }
   },
   "cell_type": "code",
   "source": "predict_random_sample(model, dataset, 2)",
   "id": "93865fe5465a75f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Настоящий тип: Токсичный | Предсказание: Токсичный\n",
      "Комментарий: БМВизм головного мозга\n",
      "\n",
      "\n",
      "\n",
      "Настоящий тип: Не токсичный | Предсказание: Не токсичный\n",
      "Комментарий: какие ваши доказательства !?\n",
      "\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T16:39:03.254713Z",
     "start_time": "2025-05-07T16:39:00.738861Z"
    }
   },
   "cell_type": "code",
   "source": "predict_programm(model, tokenizer, device)",
   "id": "cbf90c2afd481350",
   "outputs": [],
   "execution_count": 58
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
