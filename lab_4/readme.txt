BERT основан на механизме Transformer, в частности на его энкодерной части.
Transformer использует механизм self-attention, который позволяет модели учитывать контекст слов в предложении как слева