Типы задач:

-регрессия = предсказание числа
-классификация


N-layer perceptron

На вход подаётся вектор x
Умножаем вектор на веса w

Есть 3 вида слоёв:
- входные
- скрытые
- выходные


Скрытые слои всегда можно объединить
Нужны функции активации которые эту линейность убирают

---- Сигмоида ----

1/(1+e^(-x))




---- Рилу ----



---- градиентный спуск ----




Считаем ошибку
MAE = ( |y1-y1^| + |y2-y2^| ) / 2
Можно также возводить в квадрат (квадрат ошибки)
MSE = ( (y1-y1^)^2 + (y2-y2^)^2 ) / 2
Есть также менее популярные
RMSE
MAP

Все они используются в задачах регрессии


Логиты - выходные данные. Принимают любые значения

SoftMax ограничивает их
SoftMax - сумма вероятностей равна 1

Далее еспользуется лагорифм так как сумму лагорифмов намного проще дифференцировать


Из этого возникает кросс-энтропия. Когда мы считаем не по вероятности,
а запихиваем в лагорифм то что нам нужно со знаком минус
Кросс-энтропия лосс используется при классификации

Уравнение кросс-энтропии лосс из природы вышла

Кросс энтрапилосс



Градиент - производная для случаев




Альфа - Learning Rate. Почти всегда меньше 0

В оптимизаторе атом должен быть равен 3e-4

В больших моделях 5e-5

Алгоритмов для генерации весов есть множество

Теперь когда посчитали градиент

Нужно уменьшить лосс
Как применить изменения? - Граф измерений

Back propogation - прогоняем граф в обратную сторону посчитав ошибку и градиент от этой ошибки

chain rule

анти градиент смотрит в сторону уменьшения функции ошибки

Мыы строим граф вычислений так как каждую операцию монжо сделать производную

Как применять градиент к весам - разные подходы. Называются оптимизаторыы

Есть ещ спуск не по одному элементу а по батчам


Существуют разные подходы к уменьшению Learning Rate
Так как чем ближе мы к минимальному лоссу тем меньше шаги



Как понять что модель переобучена? Посмотреть на график эпох
Эпоха - это когда полностью прошлись по датасету
Всегда есть 3 выборки - train validate test
train - учимся
validate - после каждой эпохи ссмотрим как модель ошибается. Валидацию модель не выучивает

После каждой эпохи смотрим уровень ошибки на валидации

overfitting - одновременное падение ошибки на train и рост ошибки на валидации



Стэйминг
Лигмантизация











